{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7e2cf6",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "## Question 1 (a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1\n",
    "## part(a)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter= 50, tol= 1e-6):\n",
    "    x= init_\n",
    "    for i in range(n_iter):\n",
    "        delta= -learn_rate* gradient(x)\n",
    "        if np.all(np.abs(delta)<= tol):\n",
    "            break\n",
    "        x+= delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76282188",
   "metadata": {},
   "source": [
    "Part (i) $\\\\$\n",
    "The gradient1 function below will return the gradient for the equation $ x^2+ 3x +4$\n",
    "And then we are calling the gradient_descent fucntion above which will use this function to find the gradient of x and update x using the delta and return the final x basically the x value where the minina of this fucntion lies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a15c08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the min value for the fucntion for part(i) is at x =  -1.5\n"
     ]
    }
   ],
   "source": [
    "## (i) \n",
    "def gradient1(x):                                       ##function for part(i)\n",
    "    return 2*x+3\n",
    "\n",
    "print(\"the min value for the fucntion for part(i) is at x =  \"+str(gradient_descent(gradient1, 0, 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c38f6d",
   "metadata": {},
   "source": [
    " Part(ii)$\\\\$\n",
    "The gradient2 fucntion will return gradient for the equation$ x^4 â€“ 3x^2 +2x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2d8367a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the min value for the fucntion for part(ii) is at x =  -1.357\n"
     ]
    }
   ],
   "source": [
    "##(ii)\n",
    "\n",
    "def gradient2(x):\n",
    "    return 4*(x**3)- 6*x + 2\n",
    "\n",
    "print(\"the min value for the fucntion for part(ii) is at x =  \"+str(gradient_descent(gradient2, 0, 0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9af0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b55868",
   "metadata": {},
   "source": [
    "## Question 1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03069a93",
   "metadata": {},
   "source": [
    "Part(b): $$$$\n",
    "Loss function $$ \\frac{1}{n} \\sum_{i=0}^n \\left(y_i- \\left(mx_i+c\\right)\\right)^2 $$\n",
    "Partial Derivative w.r.t. m $$ \\frac{-2}{n} \\sum_{i=0}^n x_i * \\left(y_i-y_{pred}\\right) $$\n",
    "Partial Derivative w.r.t. c $$ \\frac{-2}{n} \\sum_{i=0}^n \\left(y_i-y_{pred}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cdfc3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### part (b)\n",
    " ## the gardient3 function will calculate gradient for the linear regression ax+b the function will calculate the ypred\n",
    " ## and return the value of a and b by partially derivating the function w.r.t a (for m1) , w.r.t b (for c1)\n",
    " ## the fucntion is the loss fucntion for the linear regression mentioned above.\n",
    "def gradient3(x,y,m,c):                                                  \n",
    "    ypred = m*x +c \n",
    "    n= len(x)\n",
    "    m1= float(-2/n)* np.dot((y-ypred),x)\n",
    "    c1= float(-2/n) * sum(y-ypred)\n",
    "    return m1,c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c50cf0",
   "metadata": {},
   "source": [
    "## Question 1 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "70754d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying the gradient descent on the X and Y \n",
      "Value of a 0.297\n",
      "Value of b 2.016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### part (c)\n",
    " ## the gd_lr is the function for gradient descent for the linear regression the fucntion will also take the X and Y \n",
    " ## vectors and it will use the gradient fucntion created above(gradient3) to find the gradient for given x and y     \n",
    "def gd_lr(gradient,X,Y, init_m,init_c, learn_rate, n_iter= 500, tol= 1e-6):\n",
    "    \n",
    "    m=init_m\n",
    "    c=init_c\n",
    "    x=X\n",
    "    y=Y\n",
    "    ## this loop will iterate for n_iter and will update the value for both a and b in each iteration\n",
    "    ## this uses the gradint function crrated above and finally gives the value of a and b\n",
    "    count=0\n",
    "    for i in range(n_iter):\n",
    "        count+=1\n",
    "        g1,g2=gradient(x,y,m,c)\n",
    "        delta1= -learn_rate* g1\n",
    "        delta2= -learn_rate* g2\n",
    "        if(np.all(np.abs(delta1)<= tol) and np.all(np.abs(delta2)<= tol) ):\n",
    "            break\n",
    "        m+=delta1\n",
    "        c+=delta2\n",
    "    m1= round(m*1000)/1000\n",
    "    c1= round(c*1000)/1000\n",
    "    return m1,c1\n",
    "                   \n",
    "\n",
    " ## the given data \n",
    "\n",
    "np.random.seed(0)\n",
    "X= 2.5 * np.random.randn(10000)*1.5\n",
    "res= 1.5* np.random.randn(10000) \n",
    "Y= 2+ 0.3 * X + res\n",
    " \n",
    " ## calling the gd_lr fucntion for applying the gradient descent for the above data given with learning rate=0.01\n",
    "a,b= gd_lr(gradient3,X,Y,0,0,0.01)\n",
    "print('After applying the gradient descent on the X and Y ')\n",
    "print(\"Value of a \"+ str(a))\n",
    "print(\"Value of b \"+ str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71c8ee",
   "metadata": {},
   "source": [
    "## Question 1 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1641f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying the minibatch stochastic gradient descent on the X and Y \n",
      "Value of a 0.322\n",
      "Value of b 2.02\n"
     ]
    }
   ],
   "source": [
    "# part(d)\n",
    "\n",
    "## the fucntion below is the  minibatch stochastic gradient descent function which will do the iteration and consider only\n",
    "## the batch for the gradient value and will update the value of a and b doing this iterations.\n",
    "def minibatch_sgd(gradient,X,Y, init_m,init_c, batch_size,learn_rate, n_iter= 500, tol= 1e-6):\n",
    "    \n",
    "    m=init_m\n",
    "    c=init_c\n",
    "    l= learn_rate\n",
    "    n= len(X)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        ## randomly picking n points of x and y and calling the gradint function for them\n",
    "        \n",
    "        z= np.random.permutation(n)\n",
    "        X1= X[z]\n",
    "        Y1= Y[z]\n",
    "        x= X1[0:batch_size]\n",
    "        y= Y1[0:batch_size]\n",
    "        #print(len(x1))\n",
    "        \n",
    "        g1,g2=gradient(x,y,m,c)\n",
    "        delta1= -learn_rate* g1\n",
    "        delta2= -learn_rate* g2\n",
    "        if(np.all(np.abs(delta1)<= tol) and np.all(np.abs(delta2)<= tol) ):\n",
    "            break\n",
    "        m+= delta1\n",
    "        c+=delta2\n",
    "        \n",
    "    m1= round(m*1000)/1000\n",
    "    c1= round(c*1000)/1000\n",
    "    \n",
    "    return m1,c1\n",
    "    \n",
    "a,b= minibatch_sgd(gradient3,X,Y,0,0,100,0.01,1000)\n",
    "print('After applying the minibatch stochastic gradient descent on the X and Y ')\n",
    "print(\"Value of a \"+ str(a))\n",
    "print(\"Value of b \"+ str(b))  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be534b46",
   "metadata": {},
   "source": [
    "## Question 1 (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "100ea086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying the stochastic gradient descent on the X and Y \n",
      "Value of a 0.304\n",
      "Value of b 2.004\n"
     ]
    }
   ],
   "source": [
    "#part(e)\n",
    "## for the sgd we can use the above code and calcualte while doing sgd the batch size would be 1\n",
    "a,b= minibatch_sgd(gradient3,X,Y,0,0,1,0.001,10000)\n",
    "print('After applying the stochastic gradient descent on the X and Y ')\n",
    "print(\"Value of a \"+ str(a))\n",
    "print(\"Value of b \"+ str(b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c107127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "06924a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optmal batch size is :80\n"
     ]
    }
   ],
   "source": [
    "## finding the optmal batch size\n",
    "np.random.seed(0)\n",
    "X= 2.5 * np.random.randn(10000)*1.5\n",
    "res= 1.5* np.random.randn(10000) \n",
    "Y= 2+ 0.3 * X + res\n",
    "## calculating the optmal batch size for the data \n",
    "min_batchsize=100\n",
    "min_error= float(10000000)\n",
    "for i in range(50,1000,10):\n",
    "    \n",
    "    ## finding out the gradients for the batch_szie==i and updating the batch_size\n",
    "    a,b= minibatch_sgd(gradient3,X,Y,0,0,i,0.001,100)\n",
    "    temp= (a-0.3)**2+ (b-2)**2\n",
    "    if(temp<min_error):\n",
    "        min_error= temp\n",
    "        min_batchsize=i\n",
    "\n",
    "print(\"The optmal batch size is :\"+str(min_batchsize))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "795b2006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time take by the gradient descent algo is: 12.015625\n",
      "The time take by the minibatch stochastic gradient descent algo is: 4.78125\n",
      "The time take by the stochastic gradient descent algo is: 0.5\n",
      "Ideally the time taken is SGD < miniBatch stochastic gradient descent < gradient descent\n"
     ]
    }
   ],
   "source": [
    "## For time comparisions of all the 3 algo\n",
    "## taking iter = 10000\n",
    "iter_1= 10000\n",
    "t1= time.process_time()\n",
    "a1,b1= gd_lr(gradient3,X,Y,0,0,0.001,iter_1)\n",
    "t2= time.process_time()\n",
    "print(\"The time take by the gradient descent algo is: \"+ str(t2-t1))\n",
    "t1= time.process_time()\n",
    "a2,b2= minibatch_sgd(gradient3,X,Y,0,0,100,0.001,iter_1)\n",
    "t2= time.process_time()\n",
    "print(\"The time take by the minibatch stochastic gradient descent algo is: \"+ str(t2-t1))\n",
    "t1= time.process_time()\n",
    "a3,b3 = minibatch_sgd(gradient3,X,Y,0,0,1,0.001,iter_1)\n",
    "t2= time.process_time()\n",
    "print(\"The time take by the stochastic gradient descent algo is: \"+ str(t2-t1))\n",
    "print(\"Ideally the time taken is SGD < miniBatch stochastic gradient descent < gradient descent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c51a04",
   "metadata": {},
   "source": [
    "And the optmial batch size for this data is coming out to be 80. <br>\n",
    "Here we can say that SGD is working better than the gradient descent algo in terms of time. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a047c",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42118f69",
   "metadata": {},
   "source": [
    "## Question 2 (i): \n",
    "$\\\\$\n",
    "     The probability that someone has both cold and a fever =  $ P \\left(Cold \\cap Fever \\right)$\n",
    "$$ P \\left(Cold \\cap Fever \\right) =  P\\left(Fever / Cold \\right) * P\\left(Cold\\right) $$\n",
    "     From the Bayesian network, we have\n",
    "$$   P\\left(Fever / Cold \\right) = 0.307 $$\n",
    "$$ P\\left(Cold\\right) = 0.02 \\\\$$ \n",
    "Then, \n",
    "$$\\\\ P \\left(Cold \\cap Fever \\right) = 0.307 * 0.02$$\n",
    "$$ P \\left(Cold \\cap Fever \\right) = 0.00614$$\n",
    "The probability that someone has both cold and a fever is 0.00614\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079ef0b",
   "metadata": {},
   "source": [
    "## Question 2 (ii): \n",
    "\n",
    "The probability that someone who has a cough has a cold=  $P\\left(Cold / Cough \\right)$\n",
    "$$P\\left(Cold / Cough \\right) = \\frac{ P\\left(Cough / Cold \\right) * P\\left(Cold\\right)}{P\\left(Cough\\right)}$$\n",
    "From the Bayesian network, we have\n",
    "$$ P\\left(Cold\\right) = 0.02 $$\n",
    "$$ P\\left(Cough / Cold \\right) = P\\left(Cough / \\left(Cold ,  Lung Disease\\right)\\right) * P\\left(LungDisese \\right)  + P\\left(Cough / \\left(Cold, \\overline{\\rm Lung Disease}\\right)\\right) *  P\\left(\\overline{\\rm Lung Disease} \\right) $$\n",
    "$$P\\left(LungDisese \\right) = P\\left(LungDisese/ Smoke \\right) * P\\left(Smoke \\right)  + P\\left(LungDisese/ \\overline{\\rm Smoke}\\right)* P\\left(\\overline{\\rm Smoke}\\right)   $$\n",
    "$$P\\left(LungDisese \\right) = 0.1009 * 0.2 + 0.001 * 0.8$$\n",
    "$$P\\left(LungDisese \\right) = 0.02098$$\n",
    "$$ P\\left(\\overline{\\rm Lung Disease}\\right) = 0.97902$$\n",
    "$$ P\\left(Cough / Cold \\right) = 0.7525 * 0.02098 + 0.505 * 0.97902$$\n",
    "$$ P\\left(Cough / Cold \\right) = 0.51019255$$\n",
    "\n",
    "$$ P \\left(Cough  \\right) =  P\\left(Cough / Cold,LungDisease \\right)*  P \\left(Cold \\right)*  P \\left(LungDisease \\right) +  P\\left(Cough / \\overline{\\rm Cold},LungDisease \\right)*  P \\left(\\overline{\\rm Cold} \\right)*  P \\left(LungDisease \\right)+  P\\left(Cough / Cold,\\overline{\\rm LungDisease} \\right)*  P \\left(Cold \\right)*  P \\left(\\overline{\\rm LungDisease} \\right)+  P\\left(Cough / \\overline{\\rm Cold},\\overline{\\rm LungDisease} \\right)*  P \\left(\\overline{\\rm Cold} \\right)*  P \\left(\\overline{\\rm LungDisease} \\right) $$\n",
    "\n",
    "$$ P \\left(Cough  \\right) = 0.7525* 0.02* 0.02098 + 0.505*0.98*0.02098+ 0.505*0.02*0.97902+0.01*0.98*0.97902$$\n",
    "$$P \\left(Cough  \\right) = 0.030181249$$\n",
    "\n",
    "$$P\\left(Cold / Cough \\right) = \\frac{ P\\left(Cough / Cold \\right) * P\\left(Cold\\right)}{P\\left(Cough\\right)}$$\n",
    "$$ P\\left(Cold / Cough \\right) =  \\frac{0.51091255* 0.02}{0.030181249}   $$\n",
    "$$ P\\left(Cold / Cough \\right) =  0.33856289$$\n",
    "The probability that someone who has a cough has a cold = 0.33856289\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d19fc9",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "\n",
    " MLE for the parameters of a k-sided multinomial distribution\n",
    "Observation is :\n",
    "$$ p_1 = P \\left(X_1 \\right) = \\frac{x_1}{n}$$\n",
    "$$  p_m = P \\left(X_m \\right) = \\frac{x_m}{n}$$\n",
    "the joint probability for this\n",
    "$$ L(p) = {{n}\\choose{x_1, ..., x_m}}\\prod_{i=1}^m p_i^{x_i} $$\n",
    "\n",
    "$$ L(p) = n! \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!} $$\n",
    "and the log-likelihood\n",
    "\n",
    "$$\\log L(p)= \\log \\bigg( n! \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!} \\bigg)$$\n",
    "$$\\log L(p)= \\log n! + \\log \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$\\log L(p)= \\log n! + \\sum_{i=1}^m \\log \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$\\log L(p)= \\log n! + \\sum_{i=1}^m x_i \\log p_i - \\sum_{i=1}^m \\log x_i $$\n",
    "Since $$\\sum_{i=1}^m p_i = 1$$\n",
    "$$ l'(p,\\lambda) = l(p) + \\lambda\\bigg(1 - \\sum_{i=1}^m p_i\\bigg)$$\n",
    "for $argmax_p$L(p,Î»), we will derivate this equation w.r.t. to $p_i$\n",
    "$$\\frac{\\partial}{\\partial p_i} l'(p,\\lambda) = \\frac{\\partial}\n",
    "{\\partial p_i} l(p)+ \\frac{\\partial}{\\partial p_i} \\lambda\\bigg(1 - \\sum_{i=1}^m p_i\\bigg)=0$$ \n",
    "$$\\frac{\\partial}{\\partial p_i} \\sum_{i=1}^m x_i \\log p_i\n",
    "- \\lambda \\frac{\\partial}{\\partial p_i} \\sum_{i=1}^m p_i = 0 $$\n",
    "$$ \\frac{x_i}{p_i}- \\lambda  = 0 $$\n",
    "$$ p_i = \\frac{x_i}{\\lambda} $$\n",
    "$$ p_i = \\frac{x_i}{\\lambda} $$\n",
    "$$ \\sum_{i=1}^m p_i = \\sum_{i=1}^m \\frac{x_i}{\\lambda} $$\n",
    "$$ l= \\frac{1}{\\lambda} \\sum_{i=1}^m x_i $$\n",
    "$$ \\lambda = n$$\n",
    "\n",
    "Therefore the probability distribution that maximizes the likelihood\n",
    "$$p = \\bigg(\n",
    "\\frac{x_1}{n},...,\\frac{x_m}{n}\\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb485bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
